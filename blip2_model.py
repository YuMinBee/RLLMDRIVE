"""
BLIP-2 ê¸°ë°˜ Vision-Language ëª¨ë¸
ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ë¡œ ì‹¤ì œ ì´ë¯¸ì§€ ì´í•´ ê°€ëŠ¥
"""

import torch
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from PIL import Image


class BLIP2VisionLanguageModel:
    """BLIP-2 ê¸°ë°˜ Vision-Language ëª¨ë¸"""
    
    def __init__(self, model_name="Salesforce/blip2-opt-2.7b"):
        """
        Args:
            model_name: BLIP-2 ëª¨ë¸ ì´ë¦„
                - "Salesforce/blip2-opt-2.7b" (2.7B, ê¶Œì¥)
                - "Salesforce/blip2-flan-t5-xl" (3B, ë” ì •í™•)
        """
        print(f"ğŸ”„ BLIP-2 ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print("â³ ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ(~5GB)ì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤...")
        
        # í”„ë¡œì„¸ì„œ ë° ëª¨ë¸ ë¡œë“œ
        self.processor = Blip2Processor.from_pretrained(model_name)
        self.model = Blip2ForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if not torch.cuda.is_available():
            self.model = self.model.to(self.device)
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (Device: {self.device})")
    
    def generate_response(self, image, prompt, max_length=100):
        """
        ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            image: PIL Image ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            prompt: í…ìŠ¤íŠ¸ ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸
            max_length: ìµœëŒ€ ìƒì„± ê¸¸ì´
        Returns:
            generated_text: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # ì…ë ¥ ì¤€ë¹„
        inputs = self.processor(image, prompt, return_tensors="pt").to(
            self.device, 
            torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                num_beams=5,  # Beam searchë¡œ ë” ë‚˜ì€ ê²°ê³¼
                temperature=1.0,
                do_sample=False
            )
        
        # ë””ì½”ë”©
        generated_text = self.processor.decode(outputs[0], skip_special_tokens=True)
        
        return generated_text.strip()
    
    def describe_image(self, image):
        """ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± (í”„ë¡¬í”„íŠ¸ ì—†ì´)"""
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        inputs = self.processor(image, return_tensors="pt").to(
            self.device,
            torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        with torch.no_grad():
            outputs = self.model.generate(**inputs, max_length=100)
        
        description = self.processor.decode(outputs[0], skip_special_tokens=True)
        return description.strip()
    
    def answer_question(self, image, question):
        """ì´ë¯¸ì§€ ê¸°ë°˜ ì§ˆë¬¸ ì‘ë‹µ (VQA)"""
        return self.generate_response(image, f"Question: {question} Answer:", max_length=50)


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    
    print("=" * 70)
    print("BLIP-2 Vision-Language ëª¨ë¸ ì´ˆê¸°í™”")
    print("=" * 70)
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ë” ê°€ë²¼ìš´ ë²„ì „ ì‚¬ìš©)
    # GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë©´ "Salesforce/blip2-opt-2.7b" ì‚¬ìš©
    # ë©”ëª¨ë¦¬ ë¶€ì¡±í•˜ë©´ ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
    try:
        model = BLIP2VisionLanguageModel("Salesforce/blip2-opt-2.7b")
    except Exception as e:
        print(f"âš ï¸  ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥ì„±. ë” ì‘ì€ ëª¨ë¸ ì‹œë„ ì¤‘...")
        model = BLIP2VisionLanguageModel("Salesforce/blip2-flan-t5-xl")
    
    print("\n" + "=" * 70)
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 70)
    print("\nì‚¬ìš© ë°©ë²•:")
    print("1. ì´ë¯¸ì§€ ì„¤ëª…:")
    print("   description = model.describe_image('image.jpg')")
    print("\n2. ì§ˆë¬¸ ì‘ë‹µ:")
    print("   answer = model.answer_question('image.jpg', 'What do you see?')")
    print("\n3. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸:")
    print("   response = model.generate_response('image.jpg', 'Describe the scene:')")
    
    return model


if __name__ == "__main__":
    model = main()
