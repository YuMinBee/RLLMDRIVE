"""
LLaVA-7B ê¸°ë°˜ Vision-Language ëª¨ë¸
ì´ë¯¸ì§€ ì •ë³´ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ì—¬ LLMì— ì „ë‹¬
"""

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image


class LLaVAVisionLanguageModel:
    """LLaVA ê¸°ë°˜ Vision-Language ëª¨ë¸"""
    
    def __init__(self, model_name="llava-hf/llava-1.5-7b-hf"):
        """
        Args:
            model_name: LLaVA ëª¨ë¸ ì´ë¦„
                - "llava-hf/llava-1.5-7b-hf" (7B, ê¶Œì¥)
                - "llava-hf/llava-1.5-13b-hf" (13B, ë” ì •í™•)
        """
        print(f"ğŸ”„ LLaVA ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        print("â³ ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ(~13GB)ì— ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤...")
        
        # í”„ë¡œì„¸ì„œ ë° ëª¨ë¸ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(model_name)
        
        # ë¯¸ë˜ í˜¸í™˜ì„±ì„ ìœ„í•œ processor ì„¤ì • (transformers v4.47+)
        self.processor.patch_size = 14  # ViT-Lì˜ patch size
        self.processor.vision_feature_select_strategy = 'default'  # LLaVA ê¸°ë³¸ ì „ëµ
        
        self.model = LlavaForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True
        )
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        if not torch.cuda.is_available():
            self.model = self.model.to(self.device)
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (Device: {self.device})")
        print(f"   - Vision Encoder: CLIP ViT-L")
        print(f"   - LLM: Vicuna-7B")
        print(f"   - ì´ë¯¸ì§€ í† í°: 256ê°œ (BLIP-2ë³´ë‹¤ 8ë°° ë§ìŒ)")
    
    def generate_response(self, image, prompt, max_new_tokens=200):
        """
        ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            image: PIL Image ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            prompt: í…ìŠ¤íŠ¸ ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        Returns:
            generated_text: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # LLaVA í”„ë¡¬í”„íŠ¸ í˜•ì‹
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        prompt_text = self.processor.apply_chat_template(
            conversation, 
            add_generation_prompt=True
        )
        
        # ì…ë ¥ ì¤€ë¹„
        inputs = self.processor(
            images=image,
            text=prompt_text,
            return_tensors="pt"
        ).to(self.device, torch.float16 if torch.cuda.is_available() else torch.float32)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                # temperature=0.2,
                # top_p=0.9
            )
        
        # ë””ì½”ë”© (í”„ë¡¬í”„íŠ¸ ì œì™¸)
        generated_ids = outputs[0][inputs.input_ids.shape[1]:]
        generated_text = self.processor.decode(
            generated_ids, 
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def describe_image(self, image, detail_level="detailed"):
        """
        ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
        
        Args:
            image: PIL Image ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            detail_level: "brief" (ê°„ë‹¨) ë˜ëŠ” "detailed" (ìƒì„¸)
        """
        if detail_level == "brief":
            prompt = "Briefly describe this image."
        else:
            prompt = "Describe this image in detail."
        
        return self.generate_response(image, prompt, max_new_tokens=150)
    
    def answer_question(self, image, question):
        """ì´ë¯¸ì§€ ê¸°ë°˜ ì§ˆë¬¸ ì‘ë‹µ (VQA)"""
        return self.generate_response(image, question, max_new_tokens=100)
    
    def analyze_driving_scene(self, image):
        """ììœ¨ì£¼í–‰ ì¥ë©´ ë¶„ì„ (íŠ¹í™” í”„ë¡¬í”„íŠ¸)"""
        prompt = """Analyze this driving scene and describe:
1. Road conditions (surface, weather)
2. Visible obstacles or vehicles
3. Traffic signs or signals
4. Safe driving recommendations"""
        
        return self.generate_response(image, prompt, max_new_tokens=250)


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    
    print("=" * 70)
    print("LLaVA-7B Vision-Language ëª¨ë¸ ì´ˆê¸°í™”")
    print("=" * 70)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = LLaVAVisionLanguageModel("llava-hf/llava-1.5-7b-hf")
    
    print("\n" + "=" * 70)
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 70)
    print("\nì‚¬ìš© ë°©ë²•:")
    print("1. ì´ë¯¸ì§€ ì„¤ëª…:")
    print("   description = model.describe_image('image.jpg')")
    print("\n2. ì§ˆë¬¸ ì‘ë‹µ:")
    print("   answer = model.answer_question('image.jpg', 'What do you see?')")
    print("\n3. ììœ¨ì£¼í–‰ ì¥ë©´ ë¶„ì„:")
    print("   analysis = model.analyze_driving_scene('carla_image.jpg')")
    print("\n4. ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸:")
    print("   response = model.generate_response('image.jpg', 'Your prompt')")
    
    return model


if __name__ == "__main__":
    model = main()
