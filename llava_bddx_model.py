"""
LLaVA BDD-X íŒŒì¸íŠœë‹ ëª¨ë¸
ììœ¨ì£¼í–‰ íŠ¹í™” - Berkeley DeepDrive ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµ
"""

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration, BitsAndBytesConfig
from peft import PeftModel
from PIL import Image


class LLaVABDDXModel:
    """BDD-X ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ëœ LLaVA ëª¨ë¸"""
    
    def __init__(self, 
                 base_model="llava-hf/llava-1.5-7b-hf",
                 adapter_model="Salmamoori/llava-bddx-finetuned"):
        """
        Args:
            base_model: ë² ì´ìŠ¤ LLaVA ëª¨ë¸
            adapter_model: BDD-X íŒŒì¸íŠœë‹ ì–´ëŒ‘í„°
        """
        print(f"ğŸ”„ BDD-X íŒŒì¸íŠœë‹ LLaVA ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"   - Base: {base_model}")
        print(f"   - Adapter: {adapter_model}")
        print("â³ 4-bit ì–‘ìí™” ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        # 4-bit ì–‘ìí™” ì„¤ì •
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # í”„ë¡œì„¸ì„œ ë¡œë“œ
        self.processor = AutoProcessor.from_pretrained(base_model)
        
        # ë¯¸ë˜ í˜¸í™˜ì„±ì„ ìœ„í•œ processor ì„¤ì •
        self.processor.patch_size = 14
        self.processor.vision_feature_select_strategy = 'default'
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (4-bit)
        self.model = LlavaForConditionalGeneration.from_pretrained(
            base_model,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16
        )
        
        # LoRA ì–´ëŒ‘í„° ë¡œë“œ
        print("ğŸ“¦ BDD-X ì–´ëŒ‘í„° ë¡œë”© ì¤‘...")
        self.model = PeftModel.from_pretrained(
            self.model,
            adapter_model,
            device_map="auto"
        )
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (Device: {self.device})")
        print(f"   - Vision Encoder: CLIP ViT-L")
        print(f"   - LLM: Vicuna-7B (4-bit)")
        print(f"   - íŠ¹í™”: ììœ¨ì£¼í–‰ (BDD-X ë°ì´í„°ì…‹)")
    
    def generate_response(self, image, prompt, max_new_tokens=200):
        """
        ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ì‘ë‹µ ìƒì„±
        
        Args:
            image: PIL Image ë˜ëŠ” ì´ë¯¸ì§€ ê²½ë¡œ
            prompt: í…ìŠ¤íŠ¸ ì§ˆë¬¸/í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        Returns:
            generated_text: ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        
        # LLaVA í”„ë¡¬í”„íŠ¸ í˜•ì‹
        conversation = [
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©
        prompt_text = self.processor.apply_chat_template(
            conversation, 
            add_generation_prompt=True
        )
        
        # ì…ë ¥ ì¤€ë¹„
        inputs = self.processor(
            images=image,
            text=prompt_text,
            return_tensors="pt"
        ).to(self.device, torch.float16)
        
        # ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=0.2,
                top_p=0.9
            )
        
        # ë””ì½”ë”© (í”„ë¡¬í”„íŠ¸ ì œì™¸)
        generated_ids = outputs[0][inputs.input_ids.shape[1]:]
        generated_text = self.processor.decode(
            generated_ids, 
            skip_special_tokens=True
        )
        
        return generated_text.strip()
    
    def describe_driving_scene(self, image):
        """ììœ¨ì£¼í–‰ ì¥ë©´ ì„¤ëª…"""
        prompt = "Describe this driving scene in detail."
        return self.generate_response(image, prompt, max_new_tokens=150)
    
    def analyze_road_conditions(self, image):
        """ë„ë¡œ ìƒí™© ë¶„ì„"""
        prompt = "What are the road conditions and weather in this image?"
        return self.generate_response(image, prompt, max_new_tokens=100)
    
    def detect_obstacles(self, image):
        """ì¥ì• ë¬¼ ê°ì§€"""
        prompt = "Are there any vehicles, pedestrians, or obstacles? Where are they located?"
        return self.generate_response(image, prompt, max_new_tokens=100)
    
    def recommend_action(self, image):
        """ì£¼í–‰ í–‰ë™ ì¶”ì²œ"""
        prompt = "Based on this driving scene, what should the driver do? Should they go straight, turn left, turn right, or stop?"
        return self.generate_response(image, prompt, max_new_tokens=150)


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    
    print("=" * 70)
    print("LLaVA BDD-X íŒŒì¸íŠœë‹ ëª¨ë¸ ì´ˆê¸°í™”")
    print("=" * 70)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = LLaVABDDXModel()
    
    print("\n" + "=" * 70)
    print("âœ… ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 70)
    print("\níŠ¹ì§•:")
    print("- BDD-X ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹")
    print("- ììœ¨ì£¼í–‰ ì¥ë©´ ì´í•´ íŠ¹í™”")
    print("- 4-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ")
    print("\nì‚¬ìš© ë°©ë²•:")
    print("1. ì¥ë©´ ì„¤ëª…:")
    print("   description = model.describe_driving_scene('image.jpg')")
    print("\n2. ë„ë¡œ ìƒí™©:")
    print("   conditions = model.analyze_road_conditions('image.jpg')")
    print("\n3. ì¥ì• ë¬¼ ê°ì§€:")
    print("   obstacles = model.detect_obstacles('image.jpg')")
    print("\n4. í–‰ë™ ì¶”ì²œ:")
    print("   action = model.recommend_action('image.jpg')")
    
    return model


if __name__ == "__main__":
    model = main()
